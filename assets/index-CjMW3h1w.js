const a0_0xaf501b=a0_0x4b38;function a0_0x3e99(){const _0x15940f=['14089570bGrqrx','1744834bAViWH','7060PnwGNe','4506224iqgwPd','3baMdnO','841766wjGjNV','500912RFrYBg','638330SShnHl','36hPaWAU','618qPkHbW','<!DOCTYPE\x20html>\x0a<html\x20xmlns=\x22http://www.w3.org/1999/xhtml\x22\x20lang\x20xml:lang>\x0a<head>\x0a\x20\x20<meta\x20charset=\x22utf-8\x22\x20/>\x0a\x20\x20<meta\x20name=\x22generator\x22\x20content=\x22pandoc\x22\x20/>\x0a\x20\x20<meta\x20name=\x22viewport\x22\x20content=\x22width=device-width,\x20initial-scale=1.0,\x20user-scalable=yes\x22\x20/>\x0a\x20\x20<title>AI\x20for\x20those\x20who\x20didn&#39;t\x20get\x20it</title>\x0a\x20\x20<link\x20rel=\x22stylesheet\x22\x20href=\x22./Machine\x20Learning/styles.css\x22>\x0a</head>\x0a<body>\x0a<h1\x20id=\x22ai-for-those-who-didnt-get-it.\x22>“AI”\x20for\x20those\x20who\x20didn’t\x20get\x20it.</h1>\x0a<p>I\x20have\x20never\x20gotten\x20along\x20with\x20the\x20way\x20that\x20I\x20have\x20been\x20taught\x20AI,\x20I\x20am\x20operating\x20under\x20the\x20brave\x20assumption\x20that\x20I\x20am\x20not\x20the\x20only\x20one\x20who\x20feels\x20this\x20way.\x20If\x20you\x20are\x20someone\x20who\x20just\x20wants\x20to\x20understand\x20how\x20and\x20why\x20stuff\x20works,\x20this\x20could\x20be\x20the\x20page\x20for\x20you.</p>\x0a<p>The\x20information\x20in\x20this\x20page\x20is\x20correct\x20to\x20the\x20best\x20of\x20my\x20knowledge,\x20if\x20you\x20notice\x20an\x20error\x20in\x20anything\x20I\x20have\x20written\x20here\x20please\x20do\x20not\x20hesitate\x20to\x20contact\x20me.</p>\x0a<h2\x20id=\x22neural-networks\x22>Neural\x20Networks</h2>\x0a<p>Sources:</p>\x0a<p>NN\x20just\x20lots\x20of\x20line\x20segments\x20approx\x20a\x20function:\x20<a\x20href=\x22https://www.youtube.com/@algorithmicsimplicity\x22>https://www.youtube.com/@algorithmicsimplicity</a></p>\x0a<p>Easy\x20backprop:\x20<a\x20href=\x22https://karpathy.github.io/neuralnets/\x22>https://karpathy.github.io/neuralnets/</a></p>\x0a<h3\x20id=\x22a-system-that-can-learn-any-task\x22>A\x20system\x20that\x20can\x20learn\x20any\x20task</h3>\x0a<p>Pretty\x20much\x20any\x20problem\x20can\x20be\x20modelled\x20as\x20a\x20curve\x20fitting\x20exercise,\x20as\x20a\x20function\x20from\x20inputs\x20to\x20outputs</p>\x0a<h3\x20id=\x22finding-the-function-that-made-some-data\x22>Finding\x20the\x20function\x20that\x20made\x20some\x20data</h3>\x0a<p>You\x20do\x20this\x20with\x20regression</p>\x0a<p>e.g.\x20linear\x20regression</p>\x0a<p>Assume\x20that\x20the\x20data\x20fits\x20on\x20a\x20straight\x20line</p>\x0a<p>To\x20find\x20the\x20correct\x20line,\x20we\x20need\x20to\x20tune\x20the\x20parameters\x20of\x20the\x20function\x20for\x20a\x20line\x20y=mx+c</p>\x0a<p>We\x20need\x20a\x20cost/distance\x20function\x20to\x20decide\x20how\x20good\x20a\x20line\x20fits\x20our\x20data,\x20for\x20now\x20we\x20can\x20simply\x20take\x20the\x20y\x20distance\x20from\x20every\x20data\x20point\x20to\x20its\x20function\x20prediction\x20and\x20sum\x20them\x20all\x20up\x20(the\x20sum\x20of\x20the\x20distances\x20from\x20the\x20points\x20to\x20the\x20line)</p>\x0a<p>A\x20simple\x20way\x20to\x20now\x20fine\x20tune\x20our\x20function\x20is\x20to\x20slightly\x20shift\x20each\x20of\x20our\x20parameters\x20in\x20a\x20direction\x20and\x20see\x20if\x20that\x20improves\x20the\x20cost\x20function,\x20if\x20it\x20does\x20then\x20keep\x20that\x20change,\x20we\x20keep\x20going\x20until\x20it\x20can\x20only\x20get\x20worse\x20and\x20we\x20have\x20found\x20a\x20local\x20maxima</p>\x0a<p>A\x20better\x20way\x20would\x20be\x20to\x20differentiate\x20our\x20cost\x20function,\x20and\x20use\x20that\x20to\x20find\x20the\x20input\x20values\x20for\x20the\x20minimum.\x20This\x20would\x20immediately\x20give\x20us\x20the\x20closest\x20line</p>\x0a<h3\x20id=\x22modelling-non-linear-functions\x22>Modelling\x20non-linear\x20functions</h3>\x0a<p>One\x20issue\x20with\x20this\x20though,\x20most\x20of\x20the\x20time\x20our\x20function\x20is\x20not\x20going\x20to\x20be\x20linear,\x20i.e.\x20it\x20won’t\x20follow\x20a\x20straight\x20line</p>\x0a<p>If\x20you\x20imagine\x20any\x20function\x20that\x20you\x20like\x20in\x202d,\x20a\x20wiggly\x20line,\x20you\x20can\x20imagine\x20how\x20that\x20line\x20could\x20be\x20approximated\x20by\x20a\x20series\x20of\x20straight\x20lines\x20that\x20each\x20start\x20at\x20the\x20end\x20of\x20the\x20previous\x20line\x20all\x20with\x20different\x20angles,\x20known\x20as\x20a\x20piecewise\x20linear\x20function.\x20If\x20we\x20can\x20produce\x20something\x20to\x20this\x20effect\x20and\x20weight\x20it\x20then\x20we\x20could\x20tune\x20to\x20theoretically\x20any\x20function</p>\x0a<p>Here\x20is\x20how\x20we\x20would\x20do\x20that.\x20To\x20begin\x20you\x20must\x20know\x20the\x20ReLU\x20function,\x20ReLU(x)\x20=\x20max(x,0),\x20it\x20simply\x20clamps\x20any\x20negative\x20values\x20to\x20the\x20x\x20axis.\x20If\x20you\x20imagine\x20this,\x20you\x20can\x20see\x20how\x20this\x20turns\x20your\x201\x20straight\x20line\x20y=x,\x20into\x202\x20straight\x20lines\x20connected\x20at\x20an\x20angle</p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image0.png\x22\x20style=\x22width:4.14851in;height:3.4217in\x22\x20alt=\x22A\x20red\x20line\x20in\x20a\x20square\x20AI-generated\x20content\x20may\x20be\x20incorrect.\x22\x20/></p>\x0a<p>You\x20can\x20parameterise\x20this\x20function\x20i.e.\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image1.png\x22\x20alt=\x22w_{2}\x5ctext{ReLU}\x5cleft(\x20w_{1}x\x20+\x20b_{1}\x20\x5cright)\x20+\x20b_{2}\x22\x20title=\x22w_{2}\x5ctext{ReLU}\x5cleft(\x20w_{1}x\x20+\x20b_{1}\x20\x5cright)\x20+\x20b_{2}\x22\x20/></p>\x0a<p>b1\x20decides\x20how\x20far\x20along\x20the\x20x\x20axis\x20the\x20transition\x20between\x20the\x202\x20lines\x20should\x20be</p>\x0a<p>b2\x20decides\x20how\x20far\x20along\x20the\x20y\x20axis\x20the\x20transition\x20between\x20the\x20lines\x20should\x20be\x20(therefore\x20shifting\x20both\x20lines\x20up\x20or\x20down)</p>\x0a<p>w1\x20decides\x20the\x20angle\x20of\x20the\x20non\x20clamped\x20line\x20(the\x20gradient),\x20consider\x20that\x20if\x20the\x20gradient\x20becomes\x20negative,\x20the\x20line\x20would\x20go\x20from\x20top\x20left\x20to\x20bottom\x20right\x20which\x20would\x20mean\x20the\x20right\x20line\x20would\x20now\x20be\x20clamped\x20and\x20the\x20left\x20at\x20an\x20angle.</p>\x0a<p>w2\x20further\x20controls\x20the\x20angle\x20of\x20the\x20line,\x20however\x20as\x20this\x20weight\x20multiplies\x20outside\x20of\x20ReLU,\x20it\x20does\x20not\x20get\x20clamped\x20and\x20so\x20negative\x20w2\x20allows\x20us\x20to\x20have\x20our\x20line\x20at\x20an\x20angle\x20go\x20down\x20into\x20the\x20negative\x20y\x20quadrants</p>\x0a<p>So\x20we\x20have\x20gone\x20from\x201\x20line\x20to\x202,\x20but\x20how\x20would\x20we\x20get\x20to\x203?\x20We\x20can\x20simply\x20sum\x20ReLUs\x20that\x20have\x20their\x20connection\x20point\x20at\x20different\x20x\x20values,\x20any\x20flat\x20segments\x20added\x20together\x20will\x20stay\x20flat,\x20but\x20any\x20segments\x20at\x20an\x20angle\x20will\x20superpose\x20on\x20each\x20other\x20constructive\x20or\x20destructively\x20depending\x20on\x20if\x20they\x20are\x20pointing\x20in\x20the\x20same\x20direction\x20or\x20not</p>\x0a<p>We\x20could\x20keep\x20going\x20like\x20this,\x20adding\x20another\x20ReLU\x20for\x20each\x20new\x20bend/line-segment\x20that\x20we\x20want\x20in\x20our\x20function,\x20however,\x20if\x20we\x20weight\x20this\x20sum\x20that\x20gives\x20us\x20a\x203\x20line\x20segment\x20function\x20(a\x20linear\x20combination\x20of\x20the\x202,\x202\x20line\x20seg\x20functions),\x20we\x20can\x20change\x20those\x20weights\x20to\x20generate\x20many\x20different\x203\x20line\x20segment\x20functions,\x20if\x20we\x20ReLU\x202\x20of\x20these\x20functions,\x20we\x20may\x20get\x20new\x20bends\x20at\x20different\x20points,\x20for\x20example\x20lets\x20imagine\x20we\x20these\x202\x203\x20line\x20functions\x20with\x20different\x20gradients</p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image2.png\x22\x20style=\x22width:3.76667in;height:2.88879in\x22\x20alt=\x22A\x20line\x20drawn\x20on\x20a\x20white\x20surface\x20AI-generated\x20content\x20may\x20be\x20incorrect.\x22\x20/></p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image3.png\x22\x20style=\x22width:3.55in;height:2.63423in\x22\x20alt=\x22A\x20drawing\x20of\x20a\x20line\x20AI-generated\x20content\x20may\x20be\x20incorrect.\x22\x20/></p>\x0a<p>Despite\x20the\x20bends\x20being\x20in\x20the\x20same\x20place\x20in\x20those\x202\x20functions,\x20if\x20we\x20applied\x20ReLU\x20to\x20them\x20both,\x20the\x20new\x20bends\x20would\x20be\x20at\x20different\x20points\x20on\x20the\x20x\x20axis\x20as\x20you\x20can\x20see\x20e.g.</p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image4.png\x22\x20style=\x22width:3.37507in;height:2.64167in\x22\x20alt=\x22A\x20drawing\x20of\x20a\x20line\x20AI-generated\x20content\x20may\x20be\x20incorrect.\x22\x20/></p>\x0a<p>The\x20function\x20for\x20these\x20would\x20look\x20as\x20follows</p>\x0a<p>&gt;\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image5.png\x22\x20alt=\x22ReLU(w_{1}a\x20+\x20w_{2}b)\x22\x20title=\x22ReLU(w_{1}a\x20+\x20w_{2}b)\x22\x20/></p>\x0a<p>Where\x20a\x20and\x20b\x20are\x20the\x202\x20segment\x20functions\x20we\x20started\x20with</p>\x0a<p>But\x20we\x20add\x20a\x20bias,\x20as\x20below,\x20you\x20can\x20think\x20of\x20it\x20like\x20another\x20weight\x20for\x20the\x20parameter\x20“1”</p>\x0a<p>&gt;\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image6.png\x22\x20alt=\x22\x5ctext{ReLU}\x5cleft(\x20w_{1}a\x20+\x20w_{2}b\x20+\x20c\x20\x5cright)\x22\x20title=\x22\x5ctext{ReLU}\x5cleft(\x20w_{1}a\x20+\x20w_{2}b\x20+\x20c\x20\x5cright)\x22\x20/></p>\x0a<p>So\x20by\x20making\x20this\x20step\x20(adding\x20a\x20layer),\x20new\x20parts\x20of\x20our\x20function\x20go\x20below\x20the\x20x\x20axis\x20and\x20get\x20bent,\x20we\x20can\x20continue\x20doing\x20this\x20with\x20our\x20resulting\x20functions\x20again\x20and\x20again\x20to\x20potentially\x20double\x20our\x20number\x20of\x20bends\x20each\x20time\x20(if\x202\x20bends\x20happen\x20at\x20the\x20same\x20x\x20coordinate\x20then\x20that\x20is\x20just\x201\x20bend)</p>\x0a<p>Another\x20way\x20to\x20think\x20about\x20these\x20layers\x20is,\x20like\x20I\x20said,\x20that\x20they\x20are\x20a\x20weighted\x20average\x20of\x20the\x20results\x20at\x20the\x20previous\x20layer\x20(the\x20inputs\x20if\x20we’re\x20on\x20the\x20first\x20layer),\x20so\x20our\x20resulting\x20function\x20is\x20a\x20new\x20function\x20that\x20combines\x20the\x20previous\x20layers\x20functions\x20each\x20weighted\x20by\x20their\x20importance,\x20for\x20example\x20if\x20our\x20previous\x20layer\x20had\x20modelled\x20a\x20function\x20that\x20detected\x20angry\x20language,\x20and\x20another\x20function\x20that\x20modelled\x20sarcasm,\x20our\x20next\x20layer\x20could\x20very\x20positively\x20weight\x20the\x20angry\x20language\x20and\x20negatively\x20weight\x20the\x20sarcasm\x20to\x20result\x20in\x20a\x20function\x20that\x20checks\x20for\x20when\x20someone\x20is\x20talking\x20in\x20a\x20seriously\x20angry\x20way\x20(in\x20reality\x20as\x20the\x20weights\x20are\x20tuned\x20automatically\x20the\x20functions\x20modelled\x20at\x20each\x20neuron\x20would\x20not\x20be\x20human\x20interpretable).\x20In\x20that\x20way\x20we\x20have\x20built\x20complexity\x20in\x20the\x20function\x20in\x20that\x20layer,\x20different\x20functions\x20can\x20be\x20made\x20as\x20different\x20linear\x20combinations\x20of\x20the\x20first\x20layer,\x20and\x20the\x20next\x20layer\x20can\x20add\x20to\x20that\x20complexity\x20yet\x20again,\x20from\x20this\x20we\x20can\x20see\x20that\x20all\x20a\x20neuron\x20is,\x20is\x20a\x20function\x20that\x20takes\x20in\x20the\x20entire\x20previous\x20layers\x20output\x20values\x20as\x20an\x20n-dimensional\x20input\x20point/vector,\x20and\x20returns\x20some\x20output.</p>\x0a<p>This\x20is\x20all\x20that\x20there\x20is\x20to\x20the\x20function\x20modelling\x20side\x20of\x20a\x20neural\x20network\x20using\x20ReLU,\x20we\x20can\x20stack\x20layers\x20of\x20neurons\x20to\x20produce\x20a\x20piecewise\x20function\x20(in\x20higher\x20dimensions\x20we\x20aren’t\x20splitting\x20line\x20segments\x20but\x20instead\x20plane\x20segments,\x20however\x20the\x20process\x20is\x20analogous).\x20But\x20how\x20do\x20we\x20make\x20this\x20piecewise\x20function\x20get\x20closer\x20to\x20whatever\x20data\x20we’re\x20actually\x20trying\x20to\x20model?</p>\x0a<h3\x20id=\x22learning-non-linear-functions\x22>Learning\x20non-linear\x20functions</h3>\x0a<p>Well,\x20we\x20know\x20the\x20ReLU\x20is\x20parameterised\x20by\x20the\x20weights.\x20If\x20we\x20change\x20any\x20of\x20these\x20values\x20that\x20a\x20specific\x20neuron\x20depends\x20on,\x20the\x20function\x20we’re\x20approximating\x20at\x20that\x20neuron\x20will\x20change,\x20optimising\x20these\x20weights\x20for\x20our\x20final\x20layer\x20neurons\x20to\x20have\x20the\x20best\x20output\x20result\x20(the\x20closest\x20to\x20the\x20data\x20we\x20have/function\x20we\x20want)\x20is\x20how\x20our\x20model\x20learns.\x20To\x20see\x20how\x20close\x20our\x20output\x20is\x20to\x20the\x20value\x20we\x20want,\x20we\x20need\x20samples\x20of\x20inputs\x20where\x20we\x20know\x20the\x20output\x20(training\x20data)\x20so\x20we\x20can\x20give\x20a\x20score\x20to\x20our\x20current\x20function\x20that\x20we\x20can\x20optimise</p>\x0a<p>So\x20we\x20need\x20to\x20change\x20our\x20parameters\x20to\x20make\x20our\x20complex\x20output\x20function\x20actually\x20close\x20to\x20the\x20function\x20were\x20trying\x20to\x20approximate.\x20We\x20call\x20this\x20process\x20learning\x20(as\x20the\x20model\x20is\x20learning\x20to\x20approximate\x20the\x20function).\x20One\x20way\x20we\x20could\x20do\x20this\x20is\x20to\x20simply\x20loop\x20through\x20every\x20weight,\x20change\x20it\x20slightly,\x20and\x20measure\x20what\x20that\x20does\x20to\x20our\x20final\x20score.\x20If\x20its\x20positive\x20keep\x20the\x20change,\x20if\x20its\x20negative\x20make\x20the\x20opposite\x20change,\x20just\x20like\x20we\x20did\x20initial\x20with\x20linear\x20regression.\x20This\x20would\x20gradually\x20make\x20our\x20score\x20tend\x20towards\x20being\x20better</p>\x0a<p>However\x20we\x20can\x20do\x20a\x20better\x20job,\x20it\x20would\x20be\x20great\x20if\x20we\x20could\x20differentiate\x20our\x20cost\x20function\x20and\x20find\x20the\x20minima\x20like\x20with\x20linear\x20regression,\x20the\x20issue\x20is\x20that\x20our\x20function\x20will\x20be\x20too\x20difficult\x20to\x20differentiate,\x20we\x20will\x20only\x20be\x20able\x20to\x20calculate\x20the\x20local\x20gradient\x20at\x20a\x20point,\x20not\x20the\x20global\x20gradient\x20over\x20the\x20function.</p>\x0a<p>It\x20would\x20be\x20good\x20then\x20instead,\x20to\x20utilise\x20the\x20local\x20gradient,\x20following\x20it\x20down\x20to\x20move\x20us\x20towards\x20a\x20local\x20minimum.</p>\x0a<p>If\x20you\x20imagine\x20a\x20landscape\x20with\x20lots\x20of\x20hills\x20and\x20valleys,\x20this\x20would\x20represent\x20a\x202d\x20cost\x20function,\x20if\x20you\x20placed\x20a\x20ball\x20at\x20some\x20point\x20in\x20that\x20landscape,\x20it\x20would\x20roll\x20down\x20whatever\x20hill\x20it\x20was\x20on,\x20following\x20the\x20gradient\x20to\x20reach\x20a\x20local\x20minimum,\x20where\x20it\x20isn’t\x20locally\x20possible\x20to\x20go\x20any\x20lower</p>\x0a<p>Modifying\x20random\x20weights\x20and\x20seeing\x20what\x20changes\x20is\x20like\x20moving\x20around\x20your\x20current\x20point\x20and\x20seeing\x20what\x20takes\x20you\x20lower\x20on\x20the\x20cost\x20function</p>\x0a<p>Utilising\x20the\x20local\x20gradient\x20skips\x20that\x20searching\x20as\x20we\x20already\x20know\x20which\x20direction\x20to\x20go\x20in\x20one\x20step\x20that\x20will\x20reduce\x20the\x20most\x20cost</p>\x0a<p>A\x20direction\x20will\x20just\x20be\x20a\x20vector\x20of\x20our\x20input\x20weights,\x20if\x20we\x20think\x20of\x20our\x20weights\x20as\x20coordinates\x20in\x20n-dimensional\x20space\x20this\x20will\x20become\x20clearer.\x20Our\x20gradient\x20vector\x20will\x20show\x20us\x20which\x20weights\x20are\x20contributing\x20the\x20most\x20to\x20the\x20resulting\x20error\x20and\x20by\x20how\x20much</p>\x0a<p>This\x20process\x20is\x20called\x20gradient\x20descent.</p>\x0a<p>To\x20minimise\x20our\x20cost\x20function\x20using\x20gradient\x20descent\x20on\x20our\x20network\x20we\x20start\x20by\x20defining\x20the\x20cost\x20function,\x20this\x20will\x20be\x20the\x20sum\x20of\x20the\x20squares\x20of\x20the\x20differences\x20between\x20each\x20output\x20neuron\x20and\x20its\x20expected\x20training\x20data\x20output.\x20We\x20then\x20take\x20the\x20average\x20of\x20all\x20those\x20individual\x20cost\x20values\x20for\x20each\x20sample\x20in\x20the\x20training\x20data\x20to\x20get\x20the\x20single\x20cost\x20value\x20of\x20the\x20network\x20at\x20its\x20current\x20settings.</p>\x0a<p>The\x20gradient\x20gives\x20us\x20the\x20direction\x20of\x20steepest\x20ascent,\x20so\x20we\x20need\x20to\x20negate\x20it\x20to\x20get\x20steepest\x20descent\x20(this\x20gradient\x20with\x20multivariable\x20input\x20will\x20give\x20the\x20change\x20in\x20each\x20variable\x20that\x20maximises\x20the\x20change\x20in\x20result,\x20therefore\x20this\x20will\x20be\x20a\x20vector\x20of\x20deltas\x20and\x20so\x20we\x20get\x20a\x20direction\x20vector\x20to\x20go\x20in\x20within\x20our\x20n\x20dimensional\x20space)</p>\x0a<p>To\x20perform\x20gradient\x20descent\x20efficiently\x20we\x20actually\x20do\x20stochastic\x20gradient\x20descent,\x20where\x20instead\x20of\x20using\x20all\x20our\x20samples\x20to\x20calculate\x20the\x20cost,\x20we\x20select\x20some\x20random\x20subset\x20each\x20time\x20(otherwise\x20our\x20cost\x20function\x20would\x20be\x20summing\x20potentially\x20millions\x20or\x20billions\x20of\x20terms\x20for\x20our\x20large\x20data\x20sets\x20which\x20makes\x20it\x20very\x20inefficient)</p>\x0a<p>But\x20how\x20do\x20we\x20actually\x20calculate\x20this\x20local\x20gradient?\x20Well\x20first\x20a\x20few\x20definitions.\x20The\x20process\x20of\x20passing\x20a\x20specific\x20input\x20point\x20(set\x20of\x20values)\x20through\x20layers\x20to\x20get\x20to\x20the\x20output\x20is\x20called\x20feed\x20forward\x20or\x20forward\x20propagation,\x20backpropagation\x20is\x20to\x20go\x20in\x20the\x20other\x20direction,\x20this\x20backpropagation\x20is\x20how\x20we\x20will\x20find\x20our\x20gradient</p>\x0a<p>To\x20explain\x20backpropagation\x20we\x20will\x20consider\x20a\x20single\x20training\x20input\x20instead\x20of\x20a\x20set\x20of\x20them,\x20we\x20will\x20explain\x20how\x20it\x20works\x20with\x20more\x20training\x20data\x20later</p>\x0a<p>In\x20backpropagation\x20were\x20trying\x20to\x20find\x20the\x20gradient,\x20more\x20specifically\x20the\x20derivative\x20of\x20the\x20cost\x20function\x20with\x20respect\x20to\x20the\x20weights\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image7.png\x22\x20alt=\x22\x5cfrac{d(loss)}{d(weights)}\x22\x20title=\x22\x5cfrac{d(loss)}{d(weights)}\x22\x20/></p>\x0a<p>We\x20start\x20by\x20figuring\x20out\x20the\x20gradient\x20of\x20a\x20single\x20neuron,\x20each\x20neuron\x20is\x20just\x20the\x20function\x20of\x20all\x20the\x20outputs\x20of\x20the\x20previous\x20layer,\x20weighted\x20and\x20ReLU’d.\x20This\x20is\x20a\x20mathematical\x20operation\x20and\x20therefore\x20we\x20can\x20write\x20out\x20this\x20function\x20and\x20simply\x20symbolically\x20differentiate\x20it\x20i.e.\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image8.png\x22\x20alt=\x22max(0,\x5c\x20\x5c\x20w_{0}\x20+\x20\x5c\x20w_{1}x_{1}\x5c\x20\x5c\x20\x20+\x20\x5c\x20w_{2}x_{2}\x5c\x20\x5c\x20\x20+\x20\x5c\x20\x5cldots\x5c\x20w_{n}x_{n})\x22\x20title=\x22max(0,\x5c\x20\x5c\x20w_{0}\x20+\x20\x5c\x20w_{1}x_{1}\x5c\x20\x5c\x20\x20+\x20\x5c\x20w_{2}x_{2}\x5c\x20\x5c\x20\x20+\x20\x5c\x20\x5cldots\x5c\x20w_{n}x_{n})\x22\x20/>\x20can\x20be\x20differentiated\x20piecewise\x20about\x200.\x20This\x20gives\x20us\x20a\x20function\x20by\x20which\x20we\x20can\x20get\x20the\x20gradient\x20at\x20any\x20particular\x20neuron\x20(by\x20plugging\x20the\x20values\x20at\x20that\x20neuron\x20into\x20the\x20derivative)</p>\x0a<p>We\x20want\x20to\x20find\x20the\x20gradient\x20of\x20the\x20total\x20networks\x20cost,\x20meaning\x20we\x20want\x20to\x20decrease\x20the\x20cost\x20by\x20some\x20amount\x20at\x20the\x20final\x20layer,\x20therefore\x20the\x20previous\x20layer\x20needs\x20to\x20decrease/increase\x20by\x20some\x20amount\x20to\x20make\x20that\x20happen\x20and\x20so\x20on,\x20so\x20the\x20change\x20at\x20our\x20final\x20layer\x20that\x20we\x20want\x20determines\x20the\x20change\x20we\x20need\x20to\x20see\x20in\x20the\x20previous\x20layer\x20and\x20so\x20on.\x20If\x20we\x20do\x20this\x20throughout\x20our\x20entire\x20network,\x20we\x20find\x20the\x20change\x20we\x20need\x20in\x20every\x20weight\x20to\x20get\x20the\x20change\x20in\x20the\x20output\x20we\x20want.\x20Giving\x20us\x20a\x20vector\x20of\x20weight\x20change\x20values\x20that\x20is\x20our\x20negative\x20gradient</p>\x0a<p>The\x20process\x20of\x20taking\x20our\x20wanted\x20increase\x20of\x20+1\x20and\x20going\x20back\x20through\x20the\x20network\x20calculating\x20how\x20they\x20need\x20to\x20change\x20to\x20get\x20that\x20is\x20what\x20backpropagation\x20is\x20and\x20it\x20simply\x20uses\x20the\x20chain\x20rule\x20to\x20do\x20its\x20calculation\x20(think\x20of\x20the\x20outputs\x20being\x20chained\x20to\x20their\x20inputs\x20so\x20shifting\x20the\x20final\x20output\x20by\x201\x20will\x20pull\x20the\x20weights\x20by\x20some\x20amount\x20which\x20is\x20what\x20we&#39;re\x20finding),\x20backpropagation\x20computes\x20the\x20gradients\x20of\x20the\x20cost\x20function\x20with\x20respect\x20to\x20each\x20weight,\x20and\x20gradient\x20descent\x20uses\x20these\x20gradients\x20to\x20update\x20these\x20weights</p>\x0a<p>The\x20chain\x20rule\x20is\x20just\x20the\x20statement</p>\x0a<p>Dy/Dx\x20=\x20Dy/Du\x20*\x20Du/Dx</p>\x0a<p>We\x20can\x20use\x20this\x20to\x20calculate\x20a\x20gradient\x20for\x20the\x20entire\x20network,\x20simply\x20from\x20the\x20gradients\x20of\x20each\x20individual\x20neuron\x20in\x20our\x20network</p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image9.png\x22\x20style=\x22width:2.40964in;height:1.68333in\x22\x20alt=\x22A\x20diagram\x20of\x20a\x20network\x20AI-generated\x20content\x20may\x20be\x20incorrect.\x22\x20/></p>\x0a<p>If\x20each\x20of\x20these\x20squares\x20is\x20a\x20neuron,\x20we\x20can\x20consider\x20each\x20of\x20them\x20individually,\x20and\x20combine\x20their\x20results\x20using\x20the\x20chain\x20rule</p>\x0a<p>We\x20know\x20how\x20to\x20calculate\x20the\x20gradient\x20of\x20a\x20neuron\x20with\x20respect\x20to\x20its\x20inputs\x20already,\x20So,\x20considering\x20layer\x20a\x20and\x20layer\x20b,\x20where\x20a\x20feeds\x20into\x20b,\x20if\x20we\x20calculate\x20the\x20gradient\x20of\x20A\x20with\x20respect\x20to\x20its\x20inputs,\x20and\x20we\x20calculate\x20the\x20gradient\x20of\x20B\x20with\x20respect\x20to\x20the\x20output\x20of\x20A,\x20using\x20the\x20chain\x20rule\x20we\x20can\x20compute\x20the\x20gradient\x20of\x20B\x20with\x20respect\x20to\x20the\x20inputs\x20of\x20layer\x20A</p>\x0a<p><br\x20/><img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image10.png\x22\x20alt=\x22\x5cfrac{\x5ctext{dB}}{\x5ctext{dIn}}\x20=\x20\x5cfrac{\x5ctext{dB}}{\x5ctext{dA}}\x20\x5ctimes\x20\x5cfrac{\x5ctext{dA}}{\x5ctext{dIn}}\x22\x20title=\x22\x5cfrac{\x5ctext{dB}}{\x5ctext{dIn}}\x20=\x20\x5cfrac{\x5ctext{dB}}{\x5ctext{dA}}\x20\x5ctimes\x20\x5cfrac{\x5ctext{dA}}{\x5ctext{dIn}}\x22\x20/><br\x20/></p>\x0a<p>We\x20can\x20continue\x20this\x20process\x20to\x20have\x20our\x20gradient\x20span\x20as\x20many\x20layers\x20as\x20we\x20want,\x20from\x20the\x20very\x20input\x20to\x20the\x20very\x20output\x20of\x20the\x20entire\x20network.\x20If\x20we\x20just\x20follow\x20the\x20negation\x20of\x20this\x20gradient\x20(subtract\x20its\x20components\x20from\x20our\x20weights),\x20then\x20we\x20will\x20be\x20performing\x20gradient\x20descent.</p>\x0a<p>This\x20is\x20all\x20good,\x20but\x20why\x20is\x20it\x20called\x20backpropagation?\x20Well\x20imagine\x20that\x20you\x20could\x20change\x20the\x20output,\x20and\x20it\x20would\x20change\x20the\x20inputs\x20to\x20the\x20ones\x20that\x20would\x20form\x20that\x20output.\x20If\x20it\x20were\x20that\x20way\x20around\x20then\x20the\x20inputs\x20would\x20be\x20a\x20function\x20of\x20the\x20outputs.\x20That\x20function\x20is\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image11.png\x22\x20alt=\x22input\x5c\x20\x20-\x20=\x20\x5c\x20grad(output)\x22\x20title=\x22input\x5c\x20\x20-\x20=\x20\x5c\x20grad(output)\x22\x20/>.\x20The\x20same\x20way\x20that\x20we\x20can\x20model\x20our\x20neural\x20networks\x20function\x20as\x20a\x20graph\x20of\x20our\x20neurons,\x20we\x20can\x20also\x20decompose\x20this\x20reverse\x20function\x20that\x20way\x20too</p>\x0a<p><img\x20src=\x22./Machine\x20Learning/image12.png\x22\x20style=\x22width:2.40694in;height:1.68681in\x22\x20/></p>\x0a<p>But\x20what\x20happens\x20at\x20each\x20neuron?\x20It\x20simply\x20performs\x20the\x20chain\x20rule\x20for\x20itself,\x20multiplying\x20whatever\x20gradient\x20it\x20receives\x20as\x20input\x20by\x20its\x20own\x20gradient</p>\x0a<p>But\x20what\x20do\x20we\x20do\x20when\x20we\x20receive\x20multiple\x20gradients\x20as\x20inputs?\x20Simply\x20use\x20the\x20chain\x20rule\x20on\x20all\x20of\x20them\x20and\x20add\x20up\x20the\x20resulting\x20gradients</p>\x0a<p>We\x20can\x20do\x20this\x20until\x20the\x20final\x20layer\x20(the\x20input\x20layer\x20of\x20the\x20neural\x20network),\x20here\x20we\x20just\x20subtract\x20the\x20gradient\x20we\x20get\x20and\x20we\x20have\x20performed\x20gradient\x20descent\x20using\x20backpropagation.</p>\x0a<p>One\x20last\x20point,\x20when\x20updating\x20our\x20weights,\x20the\x20size\x20of\x20each\x20update\x20is\x20scaled\x20by\x20a\x20hyperparameter\x20called\x20the\x20learning\x20rate,\x20controlling\x20this\x20is\x20important\x20as,\x20too\x20low\x20a\x20learning\x20rate\x20would\x20mean\x20we\x20would\x20have\x20to\x20perform\x20far\x20too\x20many\x20iterations\x20slowing\x20gradient\x20descent\x20dramatically,\x20but\x20too\x20high\x20a\x20rate\x20and\x20we\x20might\x20jump\x20over\x20our\x20local\x20minimum</p>\x0a<h2\x20id=\x22recurrent-neural-network-memory\x22>Recurrent\x20Neural\x20Network\x20Memory</h2>\x0a<p>We\x20want\x20a\x20neural\x20network\x20that\x20can\x20receive\x20a\x20stream\x20of\x20inputs\x20and,\x20instead\x20of\x20taking\x20them\x20each\x20individually,\x20evaluates\x20any\x201\x20input\x20using\x20the\x20additional\x20context\x20of\x20all\x20previous</p>\x0a<p>We\x20could\x20do\x20this\x20by,\x20for\x20any\x201\x20value\x20in\x20the\x20stream,\x20also\x20taking\x20all\x20previous\x20values\x20in\x20the\x20stream\x20and\x20using\x20ALL\x20of\x20that\x20as\x20the\x20current\x20network\x20input,\x20this\x20would\x20work\x20but\x20would\x20he\x20inefficient\x20as\x20we\x20would\x20have\x20to\x20continuously\x20modify\x20the\x20dimensions\x20of\x20the\x20input,\x20therefore\x20we\x20need\x20a\x20way\x20to\x20collapse\x20all\x20previous\x20values\x20into\x20a\x20constant\x20n\x20number\x20of\x20&quot;memory\x20inputs&quot;\x20for\x20any\x20value\x20in\x20the\x20stream</p>\x0a<p>Well\x20we\x20have\x20this\x20for\x20free,\x20when\x20we\x20use\x20the\x20neural\x20network\x20on\x20the\x20first\x20value,\x20we\x20get\x20neural\x20network\x20output,\x20this\x20is\x20dependent\x20on\x20just\x20the\x20first\x20value</p>\x0a<p>We\x20can\x20just\x20pipe\x20this\x20fixed\x20dimensional\x20output\x20back\x20into\x20the\x20network\x20as\x20input\x20along\x20with\x20the\x20next\x20value\x20(when\x20we\x20have\x20no\x20previous\x20state\x20we\x20simply\x20pipe\x20in\x20all\x200s),\x20so\x20now\x20the\x20output\x20will\x20be\x20dependent\x20om\x20the\x20second\x20value,\x20but\x20also\x20the\x20first\x20as\x20the\x20first\x20networks\x20output\x20was\x20dependent\x20on\x20the\x20first</p>\x0a<p>Recursively,\x20for\x20any\x20nth\x20value,\x20using\x20the\x20n-1th\x20networks\x20output\x20as\x20input\x20(the\x20same\x20network\x20but\x20its\x20previous\x20iteration),\x20we\x20add\x20dependency\x20on\x20all\x20previous\x20input\x20values,\x20and\x20therefore\x20have\x20a\x20memory\u00a0of\u00a0the\u00a0context</p>\x0a<h2\x20id=\x22lstm-neural-networks\x22>LSTM\x20Neural\x20Networks</h2>\x0a<p>RNN\x20has\x20the\x20issue\x20that,\x20over\x20time\x20as\x20more\x20context\x20is\x20remembered\x20and\x20piled\x20up,\x20the\x20network\x20becomes\x20less\x20effective\x20at\x20learning\x20new\x20things,\x20this\x20is\x20the\x20long-term\x20dependency\x20problem,\x20this\x20happens\x20as\x20in\x20RNNs,\x20the\x20gradients\x20passed\x20backwards\x20in\x20backpropagation\x20can\x20become\x20very\x20small,\x20(the\x20vanishing\x20gradient\x20problem)\x20meaning\x20our\x20model\x20struggles\x20to\x20remember\x20information\x20from\x20further\x20back\x20in\x20our\x20timeline.\x20LSTMs\x20structure\x20is\x20designed\x20to\x20maintain\x20gradients\x20better</p>\x0a<p>We\x20fix\x20this\x20in\x20Long\x20Short-Term\x20Memory\x20by\x20adding\x20internal\x20state\x20to\x20the\x20network,\x20this\x20state\x20is\x20fed\x20into\x20the\x20input\x20of\x20the\x20network,\x20just\x20like\x20previous\x20output\x20is</p>\x0a<p>This\x20state\x20is\x20modified\x20by\x203\x20gates\x20at\x20each\x20step</p>\x0a<p>The\x20forget\x20gate</p>\x0a<p>The\x20input\x20gate</p>\x0a<p>The\x20output\x20gate</p>\x0a<p>The\x20forget\x20gate\x20tells\x20us\x20what\x20information\x20that\x20is\x20stored\x20can\x20be\x20forgotten\x20as\x20it\x20is\x20irrelevant,\x20works\x20by\x20multiplying\x20the\x20state\x20by\x20a\x20value\x20between\x200\x20and\x201,\x20ranging\x20from\x20forget\x20everything\x20to\x20remember\x20it\x20all</p>\x0a<p>The\x20input\x20gate\x20tells\x20us\x20what\x20new\x20information\x20we\x20should\x20add\x20into\x20our\x20stored\x20state\x20by\x20combining\x20our\x20current\x20state\x20and\x20the\x20current\x20input</p>\x0a<p>The\x20output\x20gate\x20tells\x20us\x20which\x20parts\x20of\x20our\x20current\x20state\x20should\x20we\x20output\x20in\x20this\x20instance\x20to\x20be\x20fed\x20into\x20the\x20next\x20step</p>\x0a<p>These\x20gates\x20allow\x20our\x20model\x20to\x20selectively\x20retain\x20or\x20forget\x20information,\x20helping\x20our\x20model\x20only\x20remember\x20important\x20information\x20which\x20reduces\x20the\x20long-term\x20dependency\x20problem</p>\x0a<p>The\x20gates\x20configuration\x20is\x20learnt\x20by\x20backpropagation\x20of\x20weights\x20during\x20training\x20the\x20same\x20as\x20the\x20rest\x20of\x20our\x20model,\x20it\x20simply\x20learns\x20to\x20minimise\x20error,\x20having\x20this\x20additional\x20infrastructure\x20just\x20allows\x20our\x20network\x20the\x20system\x20to\x20learn\x20to\x20better\x20remember\x20long\x20term</p>\x0a<h2\x20id=\x22cnn-convolutional-neural-network\x22>CNN\x20(Convolutional\x20Neural\x20Network)</h2>\x0a<p>We\x20want\x20to\x20use\x20images\x20as\x20our\x20NN\x20input\x20to\x20classify\x20them,\x20however\x20with\x203\x20values\x20per\x20pixel\x20and\x20an\x20entire\x20screen\x20of\x20pixels,\x20we\x20have\x20too\x20many\x20input\x20dimensions,\x20we\x20need\x20to\x20fill\x20our\x20sample\x20space\x20quite\x20densely\x20with\x20training\x20data\x20in\x20order\x20to\x20accurately\x20interpolate\x20for\x20our\x20resulting\x20function,\x20imagine\x20filling\x201\x20dimension\x20up\x20to\x2010\x20units\x20with\x2010\x20values,\x20now\x20imagine\x202\x20dimensions,\x20filling\x20a\x2010\x20by\x2010\x20square\x20with\x20only\x2010\x20values,\x20now\x20imagine\x20a\x2010\x20by\x2010\x20by\x2010\x20cube\x20being\x20filled\x20with\x20only\x2010\x20values.\x20Each\x20time\x20we\x20add\x20a\x20dimension\x20we\x20need\x20exponentially\x20more\x20samples\x20to\x20get\x20enough\x20space\x20coverage\x20for\x20accurate\x20interpolation</p>\x0a<p>For\x20example,\x20say\x20we\x20had\x20a\x201920\x20x\x201080\x20pixel\x20image,\x20each\x20of\x20those\x20pixels\x20taking\x203\x20numbers</p>\x0a<p>1920\x20x\x201080\x20x\x203\x20=\x206220800\x20dimensions</p>\x0a<p>This\x20means\x20we\x20need\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image13.png\x22\x20alt=\x22n^{6220800}\x22\x20title=\x22n^{6220800}\x22\x20/>\x20samples\x20to\x20cover\x20the\x20space\x20where\x20n\x20is\x20the\x20number\x20of\x20samples\x20needed\x20to\x20cover\x201\x20dimension</p>\x0a<p>If\x20we\x20were\x20generous\x20and\x20said\x20we\x20only\x20needed\x202\x20samples\x20per\x20dimension,\x20that\x20would\x20still\x20leave\x20us\x20requiring\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image14.png\x22\x20alt=\x222.5\x20\x5ctimes\x2010^{1872647}\x22\x20title=\x222.5\x20\x5ctimes\x2010^{1872647}\x22\x20/>\x20samples,\x20even\x20just\x20to\x20store\x20that\x20many\x20samples,\x20if\x20every\x20sample\x20took\x20an\x20atom\x20to\x20store,\x20storing\x20all\x20the\x20samples\x20would\x20take\x20more\x20space\x20than\x20the\x20universe\x20provides,\x20in\x20fact,\x20if\x20every\x20atom\x20in\x20the\x20universe\x20each\x20contained\x20another\x20universe\x20full\x20of\x20just\x20as\x20many\x20atoms,\x20you\x20still\x20wouldn’t\x20have\x20enough\x20atoms\x20to\x20store\x20the\x20samples,\x20even\x20if\x20you\x20nested\x20these\x20universes\x2023400\x20times,\x20you\x20still\x20wouldn’t\x20have\x20enough\x20atoms.\x20This\x20is\x20evidently\x20a\x20problem</p>\x0a<p>Our\x20solution\x20is\x20to\x20convert\x20our\x20high\x20dimensional\x20data\x20into\x20lower\x20dimensional\x20data,\x20we\x20can\x20divide\x20and\x20conquer\x20logarithmically\x20dividing\x20up\x20the\x20image,\x20with\x20each\x20exponential\x20layer\x20summarising\x20some\x20information\x20about\x20the\x20image,\x20the\x20result\x20will\x20be\x20smaller\x20dimensioned\x20inputs\x20that\x20our\x20NNs\x20can\x20handle</p>\x0a<p>So\x20we\x20pick\x20some\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image15.png\x22\x20alt=\x22n\x20\x5ctimes\x20n\x22\x20title=\x22n\x20\x5ctimes\x20n\x22\x20/>\x20square\x20of\x20pixels\x20to\x20check\x20(where\x20n\x20is\x20small)\x20and\x20have\x20a\x20neural\x20network\x20generate\x20values\x20for\x20every\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image16.png\x22\x20alt=\x22n\x20\x5ctimes\x20n\x22\x20title=\x22n\x20\x5ctimes\x20n\x22\x20/>\x20section\x20of\x20our\x20full\x20image.\x20The\x20result\x20of\x20this\x20is\x20effectively\x20a\x20smaller\x20image,\x20so\x20we\x20can\x20do\x20this\x20again\x20as\x20described\x20above</p>\x0a<p>Each\x20of\x20these\x20neural\x20networks\x20at\x20each\x20layer\x20simply\x20needs\x20to\x20learn\x20a\x20function\x20that\x20translates\x20the\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image17.png\x22\x20alt=\x22n\x20\x5ctimes\x20n\x22\x20title=\x22n\x20\x5ctimes\x20n\x22\x20/>\x20values\x20that\x20it\x20sees\x20to\x20a\x20useful\x20summary\x20of\x20them\x20for\x20the\x20resulting\x20final\x20layer\x20neural\x20network\x20to\x20give\x20accurate\x20values,\x20and\x20simply\x20training\x20these\x20neural\x20networks\x20using\x20the\x20final\x20layers\x20output\x20for\x20the\x20cost\x20function\x20produces\x20exactly\x20this</p>\x0a<p>The\x20reason\x20that\x20this\x20works\x20for\x20images\x20is\x20that\x20summaries\x20of\x20close\x20sections\x20tend\x20to\x20be\x20useful\x20as\x20things\x20that\x20are\x20close\x20tend\x20to\x20be\x20related,\x20this\x20method\x20therefore\x20does\x20not\x20work\x20for\x20any\x20large\x20dimensional\x20problem</p>\x0a<h2\x20id=\x22transformers\x22>Transformers</h2>\x0a<p>Firstly,\x20how\x20can\x20we\x20encode\x20text\x20as\x20numerical\x20input?</p>\x0a<h3\x20id=\x22one-hot-encoding\x22>One-Hot\x20Encoding</h3>\x0a<p>Simply\x20list\x20every\x20possible\x20word\x20[cat,dog,mouse,rabbit,…]</p>\x0a<p>Then,\x20to\x20represent\x20some\x20word,\x20replace\x20that\x20word\x20in\x20the\x20list\x20with\x201\x20and\x20all\x20others\x20with\x200</p>\x0a<p>e.g.\x20for\x20mouse\x20[0,0,1,0,…]</p>\x0a<p>We\x20do\x20this\x20instead\x20of\x20assigning\x20the\x20words\x20to\x20natural\x20numbers\x20as\x20the\x20natural\x20numbers\x20have\x20undesired\x20properties\x20such\x20as\x20distance\x20from\x20one\x20another\x20and\x20one\x20value\x20being\x20greater\x20than\x20another,\x20we\x20don’t\x20want\x20the\x20NN\x20to\x20incorrectly\x20“pick\x20up”\x20on\x20these</p>\x0a<h3\x20id=\x22nlp-natural-language-processing\x22>NLP\x20(Natural\x20Language\x20Processing)</h3>\x0a<p>Natural\x20language\x20processing\x20is\x20another\x20example\x20of\x20a\x20high-dimensional\x20input\x20problem,\x20each\x20word\x20(or\x20any\x20token)\x20in\x20our\x20sentence\x20is\x20another\x20dimension\x20of\x20our\x20input,\x20however\x20this\x20time\x20physically\x20close\x20data\x20isn’t\x20necessarily\x20related,\x20for\x20example\x20“the\x20maintenance\x20worker\x20that\x20lived\x20down\x20the\x20street\x20from\x20my\x20grandma\x20came\x20around\x20my\x20house\x20today”,\x20in\x20this\x20sentence\x20the\x20phrases\x20“maintenance\x20worker”\x20and\x20“came\x20around\x20my\x20house”\x20are\x20closely\x20related\x20even\x20though\x20they\x20are\x20on\x20opposite\x20sides\x20of\x20the\x20sentence.\x20If\x20we\x20made\x20the\x20distance\x20assumption\x20we\x20may\x20think\x20that\x20“grandma\x20came\x20around\x20my\x20house\x20today”</p>\x0a<p>So\x20we\x20need\x20to\x20consider\x20a\x20words\x20“closeness”\x20to\x20every\x20other\x20word\x20in\x20the\x20sentence\x20regardless\x20of\x20how\x20physically\x20close\x20they\x20are</p>\x0a<p>We\x20can\x20force\x20this\x20by\x20using\x20all\x20the\x20pairs\x20of\x20words\x20in\x20the\x20sentence\x20as\x20opposed\x20to\x20just\x20the\x20words\x20in\x20the\x20sentence,\x20this\x20completely\x20removes\x20the\x20physical\x20distance\x20between\x20any\x202\x20words,\x20we\x20are\x20going\x20to\x20want\x20to\x20store\x20the\x20words\x20position\x20in\x20the\x20sentence\x20into\x20its\x20encoding\x20as\x20well\x20as\x20its\x20one-hot\x20however\x20as\x20it\x20is\x20still\x20important\x20information,\x20that\x20way\x20we\x20can\x20have\x20our\x20cake\x20and\x20eat\x20it\x20too\x20by\x20keeping\x20the\x20position\x20data\x20but\x20in\x20a\x20way\x20that\x20a\x20CNN\x20can\x20handle</p>\x0a<p>Taking\x20a\x20NN\x20and\x20using\x20these\x20pairs\x20as\x202\x20inputs\x20allows\x20us\x20to\x20“summarise”\x20them\x20into\x201\x20output,\x20we\x20can\x20then\x20pair\x20these\x20up\x20and\x20do\x20it\x20all\x20over\x20again,\x20creating\x20layers\x20similar\x20to\x20a\x20CNN</p>\x0a<p>However\x20by\x20the\x20second\x20layer,\x20we\x20are\x20applying\x20a\x20NN\x20to\x20all\x20the\x20pairs\x20of\x20pairs\x20of\x20words,\x20by\x20the\x20third\x20it\x20is\x20all\x20pairs\x20of\x20pairs\x20of\x20pairs,\x20we\x20are\x20squaring\x20the\x20number\x20of\x20inputs\x20from\x20one\x20layer\x20to\x20the\x20next.\x20We\x20need\x20to\x20squash\x20down\x20the\x20number\x20of\x20outputs\x20at\x20each\x20layer\x20to\x20be\x20the\x20same\x20as\x20its\x20number\x20of\x20inputs\x20otherwise\x20our\x20model\x20will\x20be\x20intractably\x20large</p>\x0a<p>We\x20can\x20do\x20this\x20by\x20taking\x20a\x20linear\x20combination\x20of\x20each\x20n\x20outputs,\x20this\x20combination\x20would\x20need\x20to\x20train\x20to\x20weigh\x20important\x20pairs\x20high\x20and\x20unimportant\x20pairs\x20low\x20so\x20that\x20the\x20resulting\x20sum\x20is\x20mostly\x20the\x20result\x20of\x20useful\x20information\x20and\x20not\x20noise</p>\x0a<p>But\x20for\x20that\x20to\x20happen\x20we\x20need\x20to\x20have\x20an\x20importance\x20score\x20for\x20each\x20pair,\x20this\x20is\x20the\x20job\x20of\x20yet\x20another\x20NN,\x20so\x20we\x20have\x20one\x20mapping\x20pairs\x20to\x20their\x20summaries,\x20and\x20another\x20mapping\x20them\x20to\x20an\x20importance\x20score</p>\x0a<p>This\x20is\x20known\x20as\x20self-attention\x20as\x20our\x20system\x20is\x20learning\x20what\x20pairs\x20to\x20pay\x20attention\x20to\x20and\x20the\x20sentence\x20is\x20being\x20compared\x20to\x20itSELF,\x20that\x20is\x20both\x20elements\x20of\x20the\x20comparison\x20pair\x20are\x20from\x20the\x20sentence</p>\x0a<h3\x20id=\x22generating-text\x22>Generating\x20text</h3>\x0a<p>So\x20we\x20now\x20have\x20a\x20model\x20that\x20can\x20be\x20influenced\x20by\x20relations\x20between\x20words\x20in\x20text\x20regardless\x20of\x20physical\x20distance\x20of\x20words,\x20and\x20that\x20can\x20work\x20despite\x20the\x20seemingly\x20very\x20large\x20dimensional\x20input.\x20We\x20could\x20use\x20this\x20model\x20to,\x20given\x20a\x20piece\x20of\x20text,\x20provide\x20likelihoods\x20of\x20each\x20word\x20in\x20our\x20dictionary\x20to\x20be\x20the\x20next\x20word\x20that\x20would\x20appear\x20in\x20that\x20text</p>\x0a<p>To\x20train\x20this\x20we\x20simply\x20take\x20sentences\x20that\x20are\x20truncated,\x20feed\x20them\x20in\x20and\x20check\x20the\x20models\x20predicted\x20probability\x20of\x20the\x20actual\x20word\x20that\x20came\x20next,\x20a\x20lower\x20probability\x20of\x20that\x20word\x20means\x20a\x20higher\x20cost\x20for\x20that\x20prediction\x20(if\x20we\x20see\x20a\x20large\x20enough\x20variety\x20of\x20sentences,\x20we\x20will\x20see\x20e.g.\x20I\x20feel\x20great/I\x20feel\x20wonderful,\x20and\x20even\x20though\x20one\x20will\x20get\x20a\x20higher\x20probability\x20than\x20the\x20other,\x20they\x20will\x20both\x20get\x20high\x20probabilities\x20and\x20so\x20can\x20both\x20be\x20selected)</p>\x0a<p>With\x20a\x20model\x20that\x20can\x20give\x20us\x20likelihoods\x20for\x20each\x20word\x20in\x20our\x20dictionary\x20of\x20appearing\x20next,\x20we\x20can\x20just\x20take\x20one\x20of\x20the\x20most\x20likely\x20models\x20and\x20append\x20it\x20to\x20the\x20end\x20of\x20our\x20text\x20to\x20have\x20generated\x20another\x20word\x20(One\x20of\x20the\x20most\x20likely\x20as\x20opposed\x20to\x20THE\x20most\x20likely\x20so\x20that\x20we\x20don’t\x20always\x20get\x20the\x20same\x20response\x20for\x20the\x20same\x20input)</p>\x0a<h4\x20id=\x22auto-regression\x22>Auto-Regression</h4>\x0a<p>The\x20great\x20benefit\x20of\x20generating\x20one\x20token\x20at\x20a\x20time\x20is\x20that\x20the\x20model\x20gets\x20to\x20see\x20the\x20context\x20it&#39;s\x20generating\x20into,\x20if\x20everything\x20was\x20generated\x20at\x20once\x20the\x20model\x20would\x20have\x20to\x20make\x20a\x20prediction\x20based\x20on\x20all\x20possible\x20situations\x20it\x20could\x20be\x20in,\x20for\x20example\x20all\x20possible\x204th\x20words\x20in\x20an\x20answer\x20to\x20some\x20given\x20question.\x20Generating\x20one\x20word\x20at\x20a\x20time\x20means\x20the\x204th\x20word\x20would\x20be\x20generated\x20with\x20words\x201\x20through\x203\x20set\x20in\x20stone,\x20this\x20technique\x20of\x20utilising\x20previous\x20predictions\x20for\x20future\x20predictions\x20is\x20called\x20Auto-Regression</p>\x0a<h3\x20id=\x22answering-questions\x22>Answering\x20Questions</h3>\x0a<p>So\x20far\x20our\x20help\x20bot\x20has\x20learnt\x20language\x20patterns\x20from\x20the\x20internet,\x20however,\x20just\x20because\x20it\x20looks\x20correct,\x20that\x20doesn’t\x20mean\x20that\x20it\x20is\x20useful,\x20for\x20example\x20the\x20request\x20“How\x20long\x20is\x20the\x20average\x20cat?”\x20Could\x20correctly\x20be\x20completed\x20without\x20grammatical\x20error\x20as\x20“How\x20long\x20is\x20the\x20average\x20cat?\x20Is\x20a\x20question”,\x20while\x20that\x20is\x20correct\x20that\x20is\x20not\x20useful,\x20so\x20how\x20do\x20we\x20make\x20the\x20model\x20actually\x20ask\x20questions?</p>\x0a<h4\x20id=\x22instruction-tuning\x22>Instruction\x20Tuning</h4>\x0a<p>We\x20want\x20our\x20model\x20to\x20learn\x20to\x20answer\x20question\x20responses,\x20instead\x20of\x20simply\x20generating\x20a\x20plausible\x20next\x20word,\x20so\x20far\x20it\x20has\x20just\x20been\x20learning\x20general\x20language\x20on\x20the\x20internet,\x20for\x20Instruction\x20Tuning,\x20we\x20use\x20Question-Answer\x20pairs\x20as\x20training\x20data\x20to\x20reward\x20our\x20model\x20for\x20actually\x20answering\x20questions\x20like\x20we\x20want\x20it\x20to\x20(and\x20for\x20answering\x20them\x20correctly),\x20we\x20use\x20a\x20wide\x20variety\x20of\x20ways\x20of\x20asking\x20questions\x20so\x20that\x20the\x20model\x20learns\x20those\x20patterns\x20e.g.\x20certain\x20types\x20of\x20phrases\x20and\x20their\x20common\x20responses,\x20it\x20should\x20be\x20presented\x20countless\x20diverse\x20tasks\x20so\x20that\x20it\x20can\x20more\x20easily\x20generalise\x20to\x20new\x20tasks\x20via\x20interpolation</p>\x0a<h4\x20id=\x22rlhf-reinforcement-learning-from-human-feedback\x22>RLHF\x20(Reinforcement\x20Learning\x20from\x20Human\x20Feedback)</h4>\x0a<p>To\x20make\x20our\x20model\x20more\x20useful,\x20we\x20also\x20want\x20it\x20to\x20learn\x20what\x20humans\x20actually\x20consider\x20useful,\x20therefore\x20we\x20will\x20need\x20human\x20input\x20to\x20create\x20a\x20cost/reward\x20function\x20for\x20our\x20model</p>\x0a<p>We\x20can\x20have\x20users\x20ask\x20our\x20model\x20a\x20question\x20multiple\x20times,\x20and\x20rank\x20these\x20responses\x20on\x20their\x20usefulness</p>\x0a<p>This\x20now\x20gives\x20us\x20labelled\x20data\x20that\x20we\x20can\x20train\x20a\x20neural\x20network\x20on\x20so\x20that\x20it\x20can\x20predict\x20how\x20some\x20response\x20would\x20be\x20scored\x20by\x20a\x20human,\x20this\x20NN\x20is\x20usually\x20the\x20same\x20transformer\x20model\x20as\x20our\x20ultimate\x20model\x20as\x20we\x20want\x20them\x20to\x20have\x20the\x20same\x20understanding\x20of\x20the\x20language\x20they\x20are\x20seeing,\x20however\x20it\x20now\x20is\x20learning\x20to\x20output\x20a\x20score\x20instead\x20of\x20a\x20probability\x20distribution</p>\x0a<p>We\x20can\x20use\x20this\x20neural\x20network\x20now\x20to\x20score\x20our\x20model\x20responses,\x20essentially\x20giving\x20us\x20another\x20cost\x20function\x20that\x20we\x20can\x20train\x20our\x20model\x20on,\x20this\x20should\x20improve\x20our\x20model’s\x20perceived\x20usefulness\x20by\x20our\x20users</p>\x0a<h3\x20id=\x22ppo\x22>PPO</h3>\x0a<p>We\x20need\x20to\x20ensure\x20that\x20our\x20model\x20doesn’t\x20overfit\x20to\x20certain\x20answers</p>\x0a<h4\x20id=\x22entropy-regularization\x22>Entropy\x20Regularization</h4>\x0a<p>This\x20sounds\x20tricky,\x20but\x20really\x20it\x20is\x20just\x20adding\x20some\x20random\x20value\x20to\x20our\x20cost\x20function\x20of\x20our\x20responses</p>\x0a<p>This\x20prevents\x20our\x20model\x20from\x20locking\x20into\x20answers\x20as\x20quickly,\x20it\x20instead\x20promotes\x20more\x20exploration\x20rather\x20than\x20exploitation</p>\x0a<p>The\x20random\x20addition\x20is\x20weighted\x20so\x20we\x20can\x20decide\x20how\x20much\x20to\x20explore\x20or\x20exploit</p>\x0a<h4\x20id=\x22clipping\x22>Clipping</h4>\x0a<p>When\x20changing\x20from\x20one\x20set\x20of\x20predictions\x20(for\x20our\x20next\x20word)\x20to\x20another,\x20we\x20check\x20the\x20percentage\x20difference\x20of\x20our\x20likelihood\x20before\x20the\x20change\x20and\x20our\x20likelihood\x20after,\x20If\x20this\x20percentage\x20goes\x20above\x20a\x20certain\x20threshold,\x20we\x20simply\x20reduce\x20the\x20percentage\x20down\x20to\x20that\x20threshold\x20and\x20instead\x20shift\x20our\x20previous\x20value\x20by\x20only\x20that\x20much.</p>\x0a<p>This\x20prevents\x20massive\x20changes\x20to\x20prevent\x20overfitting\x20to\x20any\x20single\x20reward,\x20it\x20also\x20prevents\x20any\x20sudden\x20changes\x20in\x20behaviour\x20of\x20the\x20model</p>\x0a<h3\x20id=\x22context-window\x22>Context\x20Window</h3>\x0a<p>When\x20you\x20ask\x20your\x20chatbot\x20something,\x20instead\x20of\x20just\x20considering\x20your\x20prompt,\x20it\x20also\x20should\x20take\x20all\x20your\x20previous\x20prompts\x20and\x20responses\x20in\x20your\x20conversation\x20for\x20context,\x20however\x20we\x20can\x20only\x20pass\x20a\x20limited\x20amount\x20of\x20tokens\x20into\x20our\x20model,\x20the\x20context\x20window\x20is\x20just\x20that\x20size,\x20the\x20amount\x20of\x20history\x20we\x20include\x20is\x20<img\x20style=\x22vertical-align:middle\x22\x20src=\x22./Machine\x20Learning/image18.png\x22\x20alt=\x22contextWindow\x20-\x20\x5c\x20promptSize\x22\x20title=\x22contextWindow\x20-\x20\x5c\x20promptSize\x22\x20/></p>\x0a<p>This\x20limited\x20size\x20means\x20that,\x20as\x20your\x20conversation\x20progresses,\x20your\x20model\x20will\x20forget\x20earlier\x20parts\x20of\x20it</p>\x0a<h3\x20id=\x22prompt-engineering-and-system-instructions\x22>Prompt\x20Engineering\x20and\x20System\x20Instructions</h3>\x0a<p>We\x20can\x20modify\x20the\x20model’s\x20behaviour\x20through\x20training,\x20but\x20we\x20can\x20also\x20modify\x20its\x20behaviour\x20by\x20changing\x20its\x20context,\x20simply\x20by\x20prepending\x20some\x20text\x20to\x20our\x20prompt.</p>\x0a<p>For\x20example,\x20adding\x20“Without\x20saying\x20any\x20swear\x20words\x20”\x20before\x20a\x20prompt\x20would\x20guide\x20the\x20behaviour\x20of\x20the\x20response</p>\x0a<p>This\x20is\x20Prompt\x20Engineering,\x20System\x20Instructions\x20are\x20simply\x20these\x20engineered\x20prompts\x20that\x20are\x20prepended\x20before\x20the\x20user\x20gets\x20to\x20the\x20prompt,\x20not\x20by\x20the\x20user\x20themselves</p>\x0a<h3\x20id=\x22cot-chain-of-thought-training\x22>CoT\x20(Chain\x20of\x20Thought\x20Training)</h3>\x0a<p>Helps\x20a\x20model\x20to\x20reason</p>\x0a<p>Simply\x20change\x20training\x20data\x20from</p>\x0a<p>Question\x20–\x20Answer</p>\x0a<p>To</p>\x0a<p>Question\x20–\x20Reasoning\x20Step\x20by\x20Step\x20–\x20Answer</p>\x0a<p>This\x20helps\x20promote\x20the\x20model\x20to\x20break\x20problems\x20down\x20into\x20steps</p>\x0a<p>This\x20should\x20go\x20hand\x20in\x20hand\x20with\x20larger\x20context\x20windows,\x20otherwise\x20our\x20model\x20will\x20forget\x20what\x20they\x20were\x20talking\x20about\x20and\x20get\x20lost</p>\x0a<h1\x20id=\x22asides\x22>Asides</h1>\x0a<h2\x20id=\x22how-can-we-generalise-the-idea-of-line-segments-to-activation-functions-other-than-relu-why-do-they-still-work\x22>How\x20can\x20we\x20generalise\x20the\x20idea\x20of\x20line\x20segments\x20to\x20activation\x20functions\x20other\x20than\x20ReLU?\x20why\x20do\x20they\x20still\x20work?</h2>\x0a<p>The\x20input\x20layer\x20in\x20the\x20network\x20just\x20describes\x20an\x20n\x20dimensional\x20point,\x20an\x20input\x20value</p>\x0a<p>The\x20first\x20hidden\x20layer\x20takes\x20a\x20weighted\x20sum\x20and\x20ReLUs\x20it.\x20These\x20weights\x20are\x20the\x20parameters\x20of\x20the\x20ReLU\x20function\x20(as\x20they\x20are\x20within\x20the\x20ReLU\x20call\x20I.e.\x20ReLU(w*x)</p>\x0a<p>This\x20parameterised\x20ReLU\x20adds\x20complexity\x20by\x20summing\x20non-linearity,\x20in\x20the\x20ReLU\x20case\x20it\x20can\x20sum\x20line\x20segments\x20with\x20different\x20pivot\x20points\x20to\x20make\x20even\x20more\x20segments,\x20in\x20other\x20activation\x20functions\x20an\x20analogous\x20complexity\x20is\x20combined.</p>\x0a<p>The\x20next\x20layer\x20does\x20this\x20same\x20thing.\x20Plugging\x20a\x20function\x20into\x20another\x20just\x20makes\x20a\x20new\x20function,\x20and\x20in\x20the\x20same\x20way\x20we\x20have\x20added\x20complexity,\x20in\x20the\x20ReLU\x20case\x20by\x20adding\x20more\x20line\x20segments.\x20A\x20more\x20complex\x20function\x20may\x20now\x20be\x20modelled</p>\x0a<p>Subsequent\x20layers\x20add\x20complexity\x20until\x20we\x20reach\x20the\x20output\x20layer.\x20This\x20is\x20the\x20most\x20complex\x20function.\x20In\x20our\x20line\x20segment\x20model,\x20it\x20is\x20the\x20function\x20with\x20the\x20most\x20line\x20segments\x20to\x20make\x20a\x20function\x20approximation,\x20in\x20general\x20we\x20are\x20adding\x20complexity\x20to\x20be\x20able\x20to\x20create\x20even\x20more\x20abstract\x20functions\x20with\x20each\x20layer</p>\x0a<p>Intuitively\x20you\x20can\x20think\x20of\x20this\x20as\x20each\x20layer\x20taking\x20the\x20large\x20amount\x20of\x20input\x20information\x20it\x20receives\x20and\x20giving\x20a\x20description\x20of\x20one\x20particular\x20part\x20of\x20it\x20(which\x20all\x20your\x20neighbouring\x20neurons\x20are\x20doing\x20about\x20different\x20parts).\x20These\x20abstractions\x20can\x20then\x20be\x20processed\x20by\x20the\x20next\x20layer\x20to\x20abstract\x20even\x20higher</p>\x0a</body>\x0a</html>\x0a','1MTBLCw'];a0_0x3e99=function(){return _0x15940f;};return a0_0x3e99();}(function(_0x3c0b97,_0x1f25c0){const _0x1f81f2=a0_0x4b38,_0x407f14=_0x3c0b97();while(!![]){try{const _0x5d4053=-parseInt(_0x1f81f2(0xcc))/0x1*(parseInt(_0x1f81f2(0xd2))/0x2)+-parseInt(_0x1f81f2(0xd1))/0x3*(parseInt(_0x1f81f2(0xd3))/0x4)+-parseInt(_0x1f81f2(0xcf))/0x5*(-parseInt(_0x1f81f2(0xca))/0x6)+parseInt(_0x1f81f2(0xce))/0x7+-parseInt(_0x1f81f2(0xd0))/0x8+-parseInt(_0x1f81f2(0xc9))/0x9*(parseInt(_0x1f81f2(0xd4))/0xa)+parseInt(_0x1f81f2(0xcd))/0xb;if(_0x5d4053===_0x1f25c0)break;else _0x407f14['push'](_0x407f14['shift']());}catch(_0x2a216f){_0x407f14['push'](_0x407f14['shift']());}}}(a0_0x3e99,0x4be3f));function a0_0x4b38(_0x146fdf,_0x387c52){const _0x3e99da=a0_0x3e99();return a0_0x4b38=function(_0x4b389f,_0x31fe1c){_0x4b389f=_0x4b389f-0xc9;let _0x2d9d09=_0x3e99da[_0x4b389f];return _0x2d9d09;},a0_0x4b38(_0x146fdf,_0x387c52);}const e=a0_0xaf501b(0xcb);export{e as default};